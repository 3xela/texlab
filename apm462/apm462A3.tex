\documentclass[12pt, a4paper]{article}
\usepackage[lmargin =0.5 in, 
rmargin=0.5in, 
tmargin=1in,
bmargin=0.5in]{geometry}
\geometry{letterpaper}
\usepackage{tikz-cd}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{cool}
\usepackage{thmtools}
\usepackage{hyperref}
\graphicspath{ }					%path to an image

%-------- sexy font ------------%
%\usepackage{libertine}
%\usepackage{libertinust1math}

%\usepackage{mlmodern}				% very nice and classic
%\usepackage[utopia]{mathdesign}
%\usepackage[T1]{fontenc}


\usepackage{mlmodern}
\usepackage{eulervm}
%\usepackage{tgtermes} 				%times new roman
%-------- sexy font ------------%


% Problem Styles
%====================================================================%


\newtheorem{problem}{Problem}


\theoremstyle{definition}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{fact}{Fact}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{question}{Question}

\newtheorem{manualprobleminner}{Problem}

\newenvironment{manualproblem}[1]{%
	\renewcommand\themanualprobleminner{#1}%
	\manualprobleminner
}{\endmanualprobleminner}

\newcommand{\penum}{ \begin{enumerate}[label=\bf(\alph*), leftmargin=0pt]}
	\newcommand{\epenum}{ \end{enumerate} }

% Math fonts shortcuts
%====================================================================%

\newcommand{\ring}{\mathcal{R}}
\newcommand{\N}{\mathbb{N}}                           % Natural numbers
\newcommand{\Z}{\mathbb{Z}}                           % Integers
\newcommand{\R}{\mathbb{R}}                           % Real numbers
\newcommand{\C}{\mathbb{C}}                           % Complex numbers
\newcommand{\F}{\mathbb{F}}                           % Arbitrary field
\newcommand{\Q}{\mathbb{Q}}                           % Arbitrary field
\newcommand{\PP}{\mathcal{P}}                         % Partition
\newcommand{\M}{\mathcal{M}}                         % Mathcal M
\newcommand{\eL}{\mathcal{L}}                         % Mathcal L
\newcommand{\T}{\mathbb{T}}                         % Mathcal T
\newcommand{\U}{\mathcal{U}}                         % Mathcal U\\
\newcommand{\V}{\mathcal{V}}                         % Mathcal V

% symbol shortcuts
%====================================================================%

\newcommand{\bd}{\partial}
\newcommand{\grad}{\nabla}
\newcommand{\lam}{\lambda}
\newcommand{\imp}{\implies}
\newcommand{\all}{\forall}
\newcommand{\exs}{\exists}
\newcommand{\delt}{\delta}
\newcommand{\ep}{\varepsilon}
\newcommand{\ra}{\rightarrow}
\newcommand{\vph}{\varphi}

\newcommand{\ol}{\overline}
\newcommand{\f}{\frac}
\newcommand{\lf}{\lfrac}
\newcommand{\df}{\dfrac}

% bracketting shortcuts
%====================================================================%
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\babs}[1]{\Big|#1\Big|}
\newcommand{\bound}{\Big|}
\newcommand{\BB}[1]{\left(#1\right)}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\artanh}{\mathrm{artanh}}
\newcommand{\Med}{\mathrm{Med}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\Range}[1]{\mathrm{range}(#1)}
\newcommand{\Null}[1]{\mathrm{null}(#1)}
\newcommand{\lan}{\langle}
\newcommand{\ran}{\rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inn}[1]{\lan#1\ran}
\newcommand{\op}[1]{\operatorname{#1}}
\newcommand{\bmat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\pmat}[1]{\begin{pmatrix}#1\end{pmatrix}}
\newcommand{\vmat}[1]{\begin{vmatrix}#1\end{vmatrix}}

\newcommand{\amogus}{{\bigcap}\kern-0.8em\raisebox{0.3ex}{$\subset$}}
\newcommand{\Note}{\textbf{Note: }}
\newcommand{\Aside}{{\bf Aside: }}
%restriction
%\newcommand{\op}[1]{\operatorname{#1}}
%\newcommand{\done}{$$\mathcal{QED}$$}

%====================================================================%


\setlength{\parindent}{0pt}      	% No paragraph indentations
\pagestyle{fancy}
\fancyhf{}							% fancy header

\setcounter{secnumdepth}{0}			% sections are numbered but numbers do not appear
\setcounter{tocdepth}{2} 			% no subsubsections in toc

%template
%====================================================================%
%\begin{manualproblem}{1}
%Spivak.
%\end{manualproblem}

%\begin{proof}[Solution]
%\end{proof}

%----------- or -----------%

%\begin{problem} 		
%\end{problem}	

%\penum
%	\item
%\epenum
%====================================================================%


\newcommand{\Course}{APM462}
\newcommand{\hwNumber}{3}

%preamble

\title{}
\author{A.N.}
\date{\today}
\lhead{\Course A\hwNumber}
\rhead{\thepage}
%\cfoot{\thepage}


%====================================================================%
\begin{document}



\begin{problem}
\end{problem}
\penum 
\item Suppose that $f$ attains a global min at $x,y$. By strict convexity for $t\in (0,1)$ we have
$$f(tx + (1-t)y) < tf(x) + (1-t) f(y).$$
This contradicts minimality of $x,y$. Therefore $f$ attains a global minimum at exactly one point. 
\item We compute the gradient of $f$ as 
$$\grad f = \bmat{4(x-3) & 2(y-5)}. $$
An interiour minimum corresponds to $\grad f =0$. This will only occur when $x=3,y=5$. This is not in the unit disk hence $f$ attains its minimum on the boundary. The boundary of $D$ is the $0$ set of the function $g(x,y) = x^2+y^2-1$. We have the following lagrange multiplier condition: 
$$ \grad f + \lambda \grad g = 0 \implies \bmat{4(x-3) \\ 2(y-5)} + \lambda \bmat{2x \\ 2y} = \bmat{0 \\ 0}$$ and $x^2+y^2 = 1$. The minimum must satisfy these equations. 
\item We compute the second order condition as
$$\grad^2 f + \lambda \grad^2 g  = \bmat{4  & 0 \\ 0 & 2 } + \lambda \bmat{2 & 0 \\ 0 & 2}.$$
Since the minimum and maximum is attained on the boundary, there must exist some $\lambda$ so that $\grad^2 f + \lambda \grad^2 g$ is positive definite since this is a necessary condition for a minimum to occur. By part $a)$ this must be the global minimum. 
\epenum
\newpage
\begin{problem}
\end{problem}
\penum 
\item We determine the regular points of the manifold defined by $h(x) = \sum_{i=1}^n x_i - 1 = 0$. We compute that $$\grad h = [1 , \dots , 1].$$
This will clearly never vanish so every feasible point is a regular point. 
\item First order conditions are given by 
$$\grad f + \lambda \grad h = 0 \implies 2 x+\lambda [1, \dots , 1] = 0. $$
It follows that for each $i$, $2x_i + \lambda = 0$ and so $x_i = -\frac{\lambda}{2}$. Plugging this into $h(x)$, we have that 
$$\sum_{i=1}^n - \frac{\lambda}{2} - 1 = -\frac{n\lambda}{2} -1= 0$$ 
and so $\lambda = -\frac{2}{n}$ and $x_i = \frac{1}{n}$. Therefore our candidate minimizer is the point $x= \pmat{\frac{1}{n} , \dots , \frac{1}{n}}$. 
\item We now determine the tangent space at our candidate point. Since every point is a regular point, we can compute the $T$ space and it must be equal to the tangent space. Vectors orthogonal to $\grad h$ take the form of $v_i = (1, 0 \dots , -1 , \dots 0)$ where the $-1$ is in the i'th spot. Thus the tangent space is spanned by $\{v_i\}$. 
\item We check positive definitess of the second order conditions: 
$$\grad^2 f + \lambda \grad^2 h = 2I -\frac{2}{n} 0 = 2I.$$
This is positive definite everywhere so we conclude that $x$ is a global minimum, since it is the unique candidate of the convex function $f$. 
\epenum
\newpage
\begin{problem}
\end{problem}
We solve the equivalent problem of minimizing $-f(x,y) = -xy$ for $h(x,y) = \frac{x^2}{a^2} + \frac{y^2}{b^2} - 1 = 0$. Additionally since we interpret $x,y$ as the lengths of a rectangle we impose that they are both positive. We use first order conditions to get that the minimizer must satisfy:
$$\grad f + \lambda \grad g = 0 \implies \bmat{-y + \lambda \frac{2x}{a^2} \\ -x + \lambda\frac{2y}{b^2}}  = \bmat{0 \\ 0 } \implies \bmat{y \\ x } = \lambda \bmat{\frac{2x}{a^2} \\ \frac{2y}{b^2}}.$$
Note that $x,y$ must both be strictly positive since we are solving for lengths. Therefore we can divide our conditions to get that 
$$\frac{y}{x}  = \frac{x}{y} \frac{b^2}{a^2} \implies y^2 = x^2 \frac{b^2}{a^2}. $$
Plugging this into the constraint we see that 
$$\frac{x^2}{a^2} + \frac{x^2}{b^2} \frac{b^2}{a^2} - 1 = 0 \implies 2\frac{x^2}{a^2} = 1 \implies x = \frac{a}{\sqrt{2}}.$$
Similarly we can compute that $y = \frac{b}{\sqrt{2}}$ and $\lambda = \frac{ab}{2}$. 
Verifying the second order necessary conditions, we have that 
$$\grad^2 f + \lambda \grad^2 h = 0+ \frac{ab}{2} \bmat{\frac{1}{a^2} & 0 \\ 0 & \frac{1}{b^2}} = \bmat{\frac{b}{a} & 0 \\ 0 & \frac{a}{b}}.$$
Since $a,b>0$ we must have that this matrix is positive definite. We conclude that $(x,y) = (\frac{a}{\sqrt{2}} , \frac{b}{\sqrt{2}})$ is a local minimum candidate and since it is the sole candidate it must be a global minimum. The value of the function will be $-\frac{ab}{2}$.  
\newpage
\begin{problem}
\end{problem}
\penum
\item We compute the gradient of $h$: 
$$\grad h = \bmat{\frac{2x}{a^2}  & \frac{2y}{b^2}}.$$
This will be $0$ if and only if $x=y=0$. This is not a feasible point however. Therefore every feasible point is regular. 
\item We solve the following optimization problem with first order conditions: 
\begin{align*}
	\begin{aligned}
		\min \quad  f(x,y) & = \frac{(x-x_0)^2}{a^2} + \frac{(y-y_0)}{b^2}
		\\ \textrm{s.t.} \quad  h(x,y) &= \frac{x^2}{a^2} + \frac{y^2}{b^2} - 1 = 0 
	\end{aligned}
\end{align*}
Taking first order conditions, we get that
$$\bmat{\frac{2(x-x_0)}{a^2} + \lambda \frac{2x}{a^2} \\ \frac{2(y-y_0)}{b^2} + \lambda \frac{2y}{b^2}}  = \bmat{0 \\ 0 }.$$
This implies that $(x-x_0) + \lambda x = 0$ and so $x = \frac{x_0}{1+ \lambda}$, and similarly $y = \frac{y_0}{1+\lambda}$. Substituting this into our constraint we get that $$\frac{x_0^2}{a^2}\frac{1}{(1+\lambda)^2} + \frac{y_0^2}{b^2} \frac{1}{(1+\lambda)^2 } = 1 \implies 1+\lambda_\pm = \pm\sqrt{\frac{x_0^2}{a^2} + \frac{y_0^2}{b^2}}.$$
The candidates will be as follows: 
$$\lambda_- = -1-\sqrt{\frac{x_0^2}{a^2} + \frac{y_0^2}{b^2}} , x_- = -\frac{x_0}{\sqrt{\frac{x_0^2}{a^2} + \frac{y_0^2}{b^2}}} , y_- = - \frac{y_0}{\sqrt{\frac{x_0^2}{a^2} + \frac{y_0^2}{b^2}}}$$ 
and 
$$\lambda_+  = -1+ \sqrt{\frac{x_0^2}{a^2} + \frac{y_0^2}{b^2}} , x_+ = \frac{x_0}{\sqrt{\frac{x_0^2}{a^2} + \frac{y_0^2}{b^2}}}, y_+ = \frac{y_0}{\sqrt{\frac{x_0^2}{a^2} + \frac{y_0^2}{b^2}}}.$$
\item Since every point is a regular point, the tangent space agrees with the T-space. Therefore the tangent space at $(x_-,y_-)$ is $T_x = \{v : v \cdot \grad h(x_-,y_-) = 0\}$. Since $$\grad h(x_-,y_-) = -\frac{1}{\sqrt{\frac{x_0^2}{a^2}}+ \frac{y_0^2}{b^2}} \bmat{\frac{2x_0}{a^2} & \frac{2y_0}{b^2}},$$
The tangent space is spanned by vectors of the form $v = \alpha \bmat{-\frac{y_0}{b^2} & \frac{x_0}{a^2}}$ for $\alpha \in \R$. Since $\grad h(x_-,y_-) = - \grad h(x_+,y_+)$, the tangent space at $(x_+,y_+)$ will be identical. 
\item Second order conditions tell us that $$\grad^2 f + \lambda \grad^2 h \geq 0.$$
For $\lambda_+,\lambda_-$ we compute:
$$\grad^2 f + \lambda_+ \grad^2h = \bmat{\frac{2}{a^2} & 0 \\ 0 & \frac{2}{b^2}} +  \left(-1+ \sqrt{\frac{x_0^2}{a^2} + \frac{y_0^2}{b^2}} \right)\bmat{\frac{2}{a^2} & 0 \\ 0 & \frac{2}{b^2}}. $$
Since by assumption $(x_0,y_0)$ is outside of the ellipse, we have that $\sqrt{\frac{x_0^2}{a^2} + \frac{y_0^2}{b^2}} >1$ and so $\lambda_+ >0$. It follows that $\grad^2 f + \lambda_+ \grad^2h$ is a diagonal matrix with positive entries and thus is positive definite. Conversely for $\lambda_-$, we have that 
$$\grad^2 f + \lambda_- \grad^2h = \bmat{\frac{2}{a^2} & 0 \\ 0 & \frac{2}{b^2}} +  \left(-1- \sqrt{\frac{x_0^2}{a^2} + \frac{y_0^2}{b^2}} \right)\bmat{\frac{2}{a^2} & 0 \\ 0 & \frac{2}{b^2}}. $$
Since $\sqrt{\frac{x_0^2}{a^2} + \frac{y_0^2}{b^2}} >1$ we must have that $\grad^2 f + \lambda_- \grad^2h$ is a diagonal matrix with negative entries and is thus negative definite. 
Since $(x_+,y_+)$ is the unique point satisfying the conditions for a global minimum on a compact set, it must be the global minimum. 
\epenum
\newpage
\begin{problem}
\end{problem}
\penum 
\item Regular points on inequality constraints are determined by when the constraint is active, i.e. when $g(x) = |x|^2 -1 = 0$. We compute the gradient as:
$$\grad g = \bmat{2x_1 , \dots , 2x_n}. $$
This vanishes if and only if $x= 0$, but this is not a point at which $g$ is active. Therefore every feasible point is regular. 
\item Kuhn-Tucker conditions for this optimization problem are given by
$$\grad f + \mu \grad g =0 \implies  Q(x-x_0) + 2\mu x = 0 \implies x = (Q+2\mu I)^{-1}Qx_0,$$
for $\mu g(x) =0$. Note that if $\mu = 0$ this would imply that $x = x_0$. But $|x_0|>1$ so this cannot occur. Therefore $g$ is active and $\mu> 0$ and so we can indeed take $x = (Q+2\mu I)^{-1}Qx_0$ as our candidate point. This also is well defined since eigenvalues of $Q$ are at least $0$, therefore the eigenvalues of $Q+2\mu I$ are at least $2\mu$ and so $Q+ 2\mu I$ is invertible. 
\item We compute the second order condition by taking hessians: 
$$\grad^2 f + \mu \grad^2 g = Q + 2\mu I.$$
Since $\mu >0$, $Q\geq 0$ it follows $Q + 2\mu I\geq 0$. This holds everywhere on $\R^n$ so we conclude that $x = (Q+2\mu I)^{-1}Qx_0$ is indeed a minimum. 
\epenum 
\newpage
\begin{problem}
\end{problem}
\penum 
\item First, note that $0 \leq x_3 \leq 1$ if and only if $g_2(x_1,x_3,x_3) = x_3(x_3-1) \leq 0$.
So we can write our optimization problem as: 
 \begin{align*}
 	\begin{aligned}
 		\min \quad  f(x) & = x_1^2+x_2^2-2x_3
 		\\ \textrm{s.t.} \quad  h(x) &= x_1+2x_2-5 = 0 
 		\\ \quad g_1(x)& = x_1+x_2-x_3 -3 \leq 0 
 		\\ \quad g_2(x) & = x_3^2 - x_3\leq 0
 	\end{aligned}
 \end{align*}
By definition a regular point is when the gradients of all the active constraints are linearly independant. We compute the gradients as:
$$\grad h = \bmat{1 & 2 & 0}, \quad \grad g_1 = \bmat{1 & 1 & -1}, \quad \grad g_2 = \bmat{0 & 0 & 2x_3 - 1}.$$
We can verify when they are linearly independent by computing the determinant of the matrix whos columns are given by $\grad h, \grad g_1, \grad g_2$. 
$$\det \left(\bmat{\grad h & \grad g_1 & \grad g_2} \right) = \det \left(\bmat{1 & 1 & 0 \\ 2 & 1 & 0 \\ 0 & -1 & 2x_3 - 1} \right) = (2x_3 - 1) - 4x_3 +2)  = -2x_3+1.$$
This will be $0$ only at $x_3 = \frac{1}{2}$. Since $g_2$ is active at $x_3 = 0,1$ this determinant will always be nonzero. Thus every point is regular. 
\item We check the first order Kuhn-Tucker conditions. We will have $4$ cases by choosing to take each $g_i$ as active or inactive. 
\begin{enumerate}[label =  \textbf{Case \Roman*:}] 
	\item $g_1,g_2$ inactive. We must have that $\mu_1 =\mu_2 = 0$. First order conditions become: 
	$$\grad f + \lambda \grad h = 0 \implies \bmat{2x_1 \\ 2x_2 \\ -2} + \lambda \bmat{1 \\ 2 \\ 0} = 0.$$
	It is clear that no such $\lambda$ can exist.   
	\item $g_1$ active, $g_2$ inactive. We must have that $\mu_1 \geq 0$, $\mu_2 = 0$ by complimentary slackness. First order conditions tell us that
	$$\grad f + \lambda \grad h + \mu_1 \grad g_1 =0 \implies \bmat{2x_1 \\ 2x_2 \\ -2} + \lambda \bmat{1 \\ 2 \\ 0} + \mu_1 \bmat{1 \\ 1 \\ -1} = 0.$$
	We can see for such to hold, $\mu_1 = -2$. This contradicts our assumption that $\mu_1 \geq 0$. 
	\item $g_1$ inactive, $g_2$ active. We must have that $\mu_1 = 0, \mu_2\geq 0$. First order conditions tell us that 
	$$\grad f + \lambda \grad h + \mu_2 \grad g_2 = \bmat{2x_1 \\ 2x_2 \\ -2} + \lambda \bmat{1 \\ 2 \\ 0} + \mu_2 \bmat{0\\ 0 \\ 2x_3 - 1} = 0,$$
	for $x_3 = 0,1$. When $x_3 -0$ we get that $\mu_2 = -2$. This cannot happen, $\mu_2$ must be nonnegative. For $x_3 = 1$ we get that $\mu_2 = 2$. This leaves us to solve:
	$$\begin{cases}
		2x_1 + \lambda & = 0.
		\\ 2x_2 + 2\lambda & = 0. 
		\\ x_1+2x_2 & = 5.
	\end{cases}$$
A quick glance tells us that $x_1 = 1,x_2 = 2, \lambda = -2$. We get a candidate minimum point $$x =(1,2,1), \lambda = -2,\mu_2 = 2. $$  
\item $g_1,g_2$ active. We must have $\mu_1,\mu_2 \geq 0$. First order conditions tell us that
$$\grad f + \lambda \grad h + \mu_1 \grad g_1 + \mu_2 \grad g_2 = 0  \implies \bmat{2x_1 \\ 2x_2 \\ -2} + \lambda \bmat{1 \\ 2 \\ 0} + \mu_1 \bmat{1 \\ 1 \\ -1} + \mu_2 \bmat{0 \\ 0 \\ 2x_3 -1}= 0 .$$
If $x_3 = 0$ we have that $\mu_1 + \mu_2 = -2$. This cannot happen since $\mu_1, \mu_2$ are both assumed to be positive. Instead if $x_3 = 1$ we have that $\mu_2 = \mu_1 + 2$. Therefore we solve the following system of equations: 
$$\begin{cases}
	2x_1 + \lambda + \mu_1 &= 0.
	\\ 2x_2 + 2\lambda + \mu_1 & = 0.
	\\ \mu_2 - \mu_1 - 2& = 0.
	\\ x_1 + 2x_2 -5 & = 0.
	\\ x_1 + x_2 - 4 & = 0.
\end{cases}$$
This will be solved by $(x_1,x_2,x_3) = (3,1,1)$ and $\lambda =2, \mu_1 = -8,\mu_2 = -6$. This cannot occur since $\mu_1,\mu_2$ are assumed to be non negative. Therefore our sole candidate point is 
$$x =(1,2,1), \lambda = -2,\mu_2 = 2. $$  
\end{enumerate}
\item We compute the second order necessary conditions at $x =(1,2,1), \lambda = -2,\mu_2 = 2. $
$$\grad^2 f + \lambda \grad^2 h + \mu_2 \grad^2 g_2= \bmat{2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 0 }-2\cdot 0 + 2 \bmat{0 & 0 & 0 \\ 0 & 0 & 0\\ 0 & 0 & 1} = 2I.$$
This is positive definite on $\R^3$ and will certainly be positive definite on $T_{x}M$. Since $x$ is the unique minimizer satisfying first and second order conditions, it must be the global minimum. 
\epenum 
\newpage
\begin{problem}
\end{problem}
\penum 
\item We wish to solve:
 \begin{align*}
	\begin{aligned}
		\min \quad  f(x) & = \frac{1}{2}\norm{Ax -b}^2
		\\ \textrm{s.t.} \quad  g(x) &= \norm{x}^2 - 1  \leq 0
	\end{aligned}
\end{align*}
If $g$ is active we must have that for $\mu\geq 0$, 
$$\grad f + \mu \grad g = 0 \implies A^TAx - A^Tb + 2\mu x = 0 \implies x = (A^TA + 2\mu I)^{-1}A^Tb.$$
If $g$ is inactive , $\mu = 0$ and we know from a previous problem set that $$\grad f = 0 \implies x = (A^TA)^{-1}A^Tb.$$
We recover the inactive case from the active case by setting $\mu =0$ so without loss of generality write 
$$x = (A^TA + 2\mu I)^{-1}A^Tb.$$
Since $A^TA$ is rank $n$ it has no nonzero eigenvalues, and since $\mu \geq 0$ $A^TA + 2\mu I$ will have no nonzero eigenvalues. Therefore it makes sense to take inverses. 
\item Taking second order derivatives, we have that
$$\grad^2 f + \mu \grad^2 g = A^TA + 2\mu I. $$
Since $\mu \geq 0$ we have that $A^TA + 2\mu I$ will be positive definite on $\R^n$, and therefore on the tangent space at any point. We conclude that our point is the unique local minimizer by convexity. 
\epenum 

\end{document}
