\documentclass[12pt, a4paper]{article}
\usepackage[lmargin =0.5 in, 
rmargin=0.5in, 
tmargin=1in,
bmargin=0.5in]{geometry}
\geometry{letterpaper}
\usepackage{tikz-cd}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{cool}
\usepackage{thmtools}
\usepackage{pythonhighlight}
\usepackage{hyperref}
\graphicspath{ }					%path to an image

%-------- sexy font ------------%
%\usepackage{libertine}
%\usepackage{libertinust1math}

%\usepackage{mlmodern}				% very nice and classic
%\usepackage[utopia]{mathdesign}
%\usepackage[T1]{fontenc}


\usepackage{mlmodern}
\usepackage{eulervm}
%\usepackage{tgtermes} 				%times new roman
%-------- sexy font ------------%


% Problem Styles
%====================================================================%


\newtheorem{problem}{Problem}


\theoremstyle{definition}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{fact}{Fact}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{question}{Question}

\newtheorem{manualprobleminner}{Problem}

\newenvironment{manualproblem}[1]{%
	\renewcommand\themanualprobleminner{#1}%
	\manualprobleminner
}{\endmanualprobleminner}

\newcommand{\penum}{ \begin{enumerate}[label=\bf(\alph*), leftmargin=0pt]}
	\newcommand{\epenum}{ \end{enumerate} }

% Math fonts shortcuts
%====================================================================%

\newcommand{\ring}{\mathcal{R}}
\newcommand{\N}{\mathbb{N}}                           % Natural numbers
\newcommand{\Z}{\mathbb{Z}}                           % Integers
\newcommand{\R}{\mathbb{R}}                           % Real numbers
\newcommand{\C}{\mathbb{C}}                           % Complex numbers
\newcommand{\F}{\mathbb{F}}                           % Arbitrary field
\newcommand{\Q}{\mathbb{Q}}                           % Arbitrary field
\newcommand{\PP}{\mathcal{P}}                         % Partition
\newcommand{\M}{\mathcal{M}}                         % Mathcal M
\newcommand{\eL}{\mathcal{L}}                         % Mathcal L
\newcommand{\T}{\mathbb{T}}                         % Mathcal T
\newcommand{\U}{\mathcal{U}}                         % Mathcal U\\
\newcommand{\V}{\mathcal{V}}                         % Mathcal V

% symbol shortcuts
%====================================================================%

\newcommand{\bd}{\partial}
\newcommand{\grad}{\nabla}
\newcommand{\lam}{\lambda}
\newcommand{\imp}{\implies}
\newcommand{\all}{\forall}
\newcommand{\exs}{\exists}
\newcommand{\delt}{\delta}
\newcommand{\ep}{\varepsilon}
\newcommand{\ra}{\rightarrow}
\newcommand{\vph}{\varphi}

\newcommand{\ol}{\overline}
\newcommand{\f}{\frac}
\newcommand{\lf}{\lfrac}
\newcommand{\df}{\dfrac}

% bracketting shortcuts
%====================================================================%
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\babs}[1]{\Big|#1\Big|}
\newcommand{\bound}{\Big|}
\newcommand{\BB}[1]{\left(#1\right)}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\artanh}{\mathrm{artanh}}
\newcommand{\Med}{\mathrm{Med}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\Range}[1]{\mathrm{range}(#1)}
\newcommand{\Null}[1]{\mathrm{null}(#1)}
\newcommand{\lan}{\left\langle}
\newcommand{\ran}{\right\rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inn}[1]{\lan#1\ran}
\newcommand{\op}[1]{\operatorname{#1}}
\newcommand{\bmat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\pmat}[1]{\begin{pmatrix}#1\end{pmatrix}}
\newcommand{\vmat}[1]{\begin{vmatrix}#1\end{vmatrix}}

\newcommand{\amogus}{{\bigcap}\kern-0.8em\raisebox{0.3ex}{$\subset$}}
\newcommand{\Note}{\textbf{Note: }}
\newcommand{\Aside}{{\bf Aside: }}
%restriction
%\newcommand{\op}[1]{\operatorname{#1}}
%\newcommand{\done}{$$\mathcal{QED}$$}

%====================================================================%


\setlength{\parindent}{0pt}      	% No paragraph indentations
\pagestyle{fancy}
\fancyhf{}							% fancy header

\setcounter{secnumdepth}{0}			% sections are numbered but numbers do not appear
\setcounter{tocdepth}{2} 			% no subsubsections in toc

%template
%====================================================================%
%\begin{manualproblem}{1}
%Spivak.
%\end{manualproblem}

%\begin{proof}[Solution]
%\end{proof}

%----------- or -----------%

%\begin{problem} 		
%\end{problem}	

%\penum
%	\item
%\epenum
%====================================================================%


\newcommand{\Course}{APM462}
\newcommand{\hwNumber}{4}

%preamble

\title{}
\author{A.N.}
\date{\today}
\lhead{\Course A\hwNumber}
\rhead{\thepage}
%\cfoot{\thepage}


%====================================================================%
\begin{document}
	
	
	
	\begin{problem}
		% problem number 1
	\end{problem}
	First we compute the gradient of $Q_i$: 
	$$ \grad Q_i(w) = \bmat{2(w_1 + w_2 \cdot x_i - y_i ) \\ 2x_i(w_1 + w_w \cdot x_i - y_i)} .$$
	Using $w_0 = \bmat{0.4 \\ 0.1}, \ep = 0.05$ at the points $(0,0),(1,0), (0,1), (1,1) $ we compute: 
	\begin{align*}
		w_1 & = w_0 - \ep\grad Q_i(w_0) = \bmat{0.4 \\ 0.1} - \ep \bmat{ 2 \cdot (0.4+ 0 \cdot 0.4 - 0) \\ 0} = \bmat{0.36 \\ 0.1} 
		\\ w_2 & = w_1 - \ep \grad Q_i ( w_1) = \bmat{0.36 \\ 0.1 } - \ep \bmat{2(0.36 + 0.1 \cdot 0 - 1) \\ 0.1}  = \bmat{0.424 \\ 0}
		\\ w_3 & = w_2 -\ep \grad Q_i (w_2) = \bmat{0.424 \\ 0.1 } - \ep \bmat{2 ( 0.424 + 1 \cdot 0.1 - 0) \\ 2(0.424 + 1 \cdot 0.1 - 0)} = \bmat{0.3716 \\ 0.0476}
		\\ w_4 & = w_3 - \ep \grad Q_i (w_3)  = \bmat{0.3716 \\ 0.0476} - \ep \bmat{2 (0.376 + 1 \cdot 0.0476  - 1) \\ 2 (0.3716 + 1 \cdot 0.0476 - 1)} = \bmat{0.42968 \\ 0.10568}
	\end{align*}
	Using attatched code, we compute out the remainder: 
	\begin{python}
		import numpy as np
		
		def grad_Q(w, data_point):
		x_comp = 2*(w[0] + w[1] * data_point[0] - data_point[1])
		y_comp = 2*data_point[0]*(w[0] + w[1]*data_point[0] - data_point[1])
		return np.array([x_comp, y_comp])
		
		learning_rate = 0.05
		w_0 = [0.4,0.1]
		
		data_point = [[0.0,0.0],[0.0,1.0],[1.0,0.0],[1.0,1.0]]
		points = []
		for i in range(0,4):
		points.extend(data_point)
		
		double_u=[w_0]
		
		for i in range(len(points)):
		w_new = double_u[i] - learning_rate*grad_Q(double_u[i] , points[i])
		double_u.append(w_new)
		
		print(double_u)
		print(len(double_u))
	\end{python}
	The output of which is:
	\begin{align*}
		w_0 & =[0.4, 0.1]
		\\ w_1 & =[0.36, 0.1 ] 
		\\ w_2 & =[0.424, 0.1  ] 
		\\ w_3 & =[0.3716, 0.0476] 
		\\ w_4 & =[0.42968, 0.10568] 
		\\ w_5 & =[0.386712, 0.10568 ]
		\\ w_6 & =[0.4480408, 0.10568  ] 
		\\ w_7 & =[0.39266872, 0.05030792] 
		\\ w_8 & =[0.44837106, 0.10601026] 
		\\ w_9 & =[0.40353395, 0.10601026] 
		\\ w_{10} & =[0.46318056, 0.10601026] 
		\\ w_{11} & =[0.40626147, 0.04909117] 
		\\ w_{12} & =[0.46072621, 0.10355591]
		\\ w_{13} & =[0.41465359, 0.10355591]
		\\ w_{14} & =[0.47318823, 0.10355591] 
		\\ w_{15} & =[0.41551382, 0.0458815 ] 
		\\ w_{16} & =[0.46937428, 0.09974196]
	\end{align*}
	\newpage
	\begin{problem}
		% problem number 2
	\end{problem}
	Starting with $x_0 = 0.5$ $\ep = 0.05$, $f^\prime(x) = 6x^5+2x-6$ we iterate:
	\begin{align*}
		x_1 & = 0.5  - 0.05\cdot f^\prime(0.5) = 0.74
		\\ x_2 & = 0.74 -0.05 f^\prime(0.74 ) = 0.899
		\\ x_3 & =0.899 - 0.05 f^\prime(0.899) = 0.933
		\\ x_4 & = 0.933 - 0.05 f^\prime(0.933) = 0.927
		\\ x_5 & = 0.927 - 0.05 f^\prime(0.927) = 0.929
	\end{align*}
	This aligns with what we expect. 
	\newpage
	\begin{problem}
		% problem number 3
	\end{problem}
	First given $Q = \bmat{4 & \sqrt{3} \\ \sqrt{3} & 3}$, we compute the eigenvalues $\lambda_0 = \frac{ 7+ \sqrt{13} }{ 2 }$, $\lambda_1 = \frac{ 7-\sqrt{13} }{ 2 }$ with corresponding eigenvectors $v_0 = \bmat{ \frac{ 1+ \sqrt{13} }{2\sqrt{3} }\\ 1}, v_1 = \bmat{ \frac{ 1-\sqrt{13} }{2\sqrt{3} }\\ 1}$.
	We now perform the conjugate directions algorithm starting with $x_0 = (10,-20)$.  
	We set 
	$$g_1 = Qx_0 = \bmat{4 & \sqrt{3} \\ \sqrt{3} & 3} \bmat{10 \\ -20}=\bmat{40 - 20 \sqrt{3} \\ 10\sqrt{3} -60} .$$
	Therefore
	$$ \alpha_0 = -\frac{ g_1\cdot v_0 }{ v_0^T Q v_0 }= - \frac{ \bmat{40 - 20 \sqrt{3} \\ 10\sqrt{3} -60} \cdot \bmat{ \frac{ 1+ \sqrt{13} }{ 2\sqrt{3} }\\ 1}}{\bmat{ \frac{ 1+ \sqrt{13} }{ 2\sqrt{3}} & 1}  \bmat{4 & \sqrt{3} \\ \sqrt{3} & 3} \bmat{ \frac{ 1+\sqrt{13} }{ 2\sqrt{3} } \\ 1} } =- \frac{ 50\sqrt{3} + 20\sqrt{39} + 30 \sqrt{13} -210 }{ 3 }.	$$ 
	Therefore
	$$ x_1 = x_0 + \alpha_0 v_0 = \bmat{ \frac{ 65+20\sqrt{39} - 5 \sqrt{13} }{ 13 }\\ \frac{ -130\sqrt{3} -30\sqrt{13} - 10 \sqrt{39}  }{ 13\sqrt{3} } }. $$ 
	We now find $g_1 = Qx_1$:
	$$ g_1 = Qx_1 = \bmat{4 & \sqrt{3} \\ \sqrt{3} & 3}  \bmat{ \frac{ 65+20\sqrt{39} - 5 \sqrt{13} }{ 13 }\\ \frac{ -130\sqrt{3} -30\sqrt{13} - 10 \sqrt{39}  }{ 13\sqrt{3} } }= \bmat{ \frac{ 260+ 70 \sqrt{39} - 50 \sqrt{13} - 130 \sqrt{3} }{ 13 }\\ \frac{ 65\sqrt{3} + 30 \sqrt{13} - 35 \sqrt{39} - 390 }{ 13 } } .$$
	We compute $\alpha_1$, 
	$$ \alpha_1= - \frac{ g_1 \cdot v_1}{v_1 Q v_1 }= \frac{ 10\sqrt{39} + 30 \sqrt{13} + 130\sqrt{3}  }{ 13\sqrt{3} } $$ 
	Finally, we compute 
	$$x_2 = x_1 + \alpha_1 v_1=\bmat{ \frac{ 260+ 70 \sqrt{39} - 50 \sqrt{13} - 130 \sqrt{3} }{ 13 }\\ \frac{ 65\sqrt{3} + 30 \sqrt{13} - 35 \sqrt{39} - 390 }{ 13 } }  +   \frac{ 10\sqrt{39} + 30 \sqrt{13} + 130\sqrt{3}  }{ 13\sqrt{3} } \bmat{ \frac{ 1-\sqrt{13} }{2\sqrt{3} }\\ 1} = 0.$$
	Therefore the minimum is 0, which is what we expect since this is of the form $x^TQx$. 
	\newpage
	\begin{problem}
		% problem number 4
	\end{problem}
	\penum 
	\item We perform gradient descent on $f(w)$ with learning rate $\alpha_k  = \frac{|g(w_k)|}{g(w_k) Q g(w_k)}$, where $g(w_k) = \grad E(w)$
	We compute $$\grad E(w) = \bmat{101(101w_1 + 101w_2 - 1)  + 101(101w_1 + 99w_2) \\ 101(101w_1 + 101w_2 - 1)  + 99(101w_1 + 99 w_2)}.$$
	Similarly we compute the hessian $Hess(E)$ as
	$$Hess(E) = \bmat{2\cdot 101^2 & 101^2 + 101\cdot99\\ 101^2 + 101\cdot 99 & 101^2 + 99^2}.$$
	The following code implements gradient descent for 5 steps. 
	\begin{python}
		import numpy as np
		from numpy import linalg
		
		Q =np.array([[2*101*101, 101*101+101*99],[101*101+101*99, 101*101 + 99*99]])
		
		def grad_E(input):
		x = 101*(101*input[0] + 101*input[1] - 1) + 101*(101*input[0] + 99*input[1])
		y = 101*(101*input[0] + 101*input[1] - 1) + 99*(101*input[0] + 99*input[1])
		return np.array([x,y])
		
		def learning_rate(input):
		num =np.sum(grad_E(input) * grad_E(input))
		denom = np.dot(np.dot(input.T, Q), input)
		return num/denom
		w_0 = np.array([10,10])
		
		double_u = [w_0]
		
		for i in range(0,6):
		w_new = double_u[i] - learning_rate(double_u[i]) * grad_E(double_u[i])
		double_u.append(w_new)
		print(double_u)
	\end{python}
	The output is:
	\begin{align*}
		x_0 &= [10, 10]
		\\ x_1 & = [-1.63921428e+10, -1.62306116e+10]
		\\ x_2 & = [2.67585307e+19, 2.64949062e+19]
		\\ x_3 & = [-4.36806684e+28, -4.32503275e+28]
		\\ x_4 & = [7.13043933e+37, 7.06019041e+37]
		\\ x_5 & =[-1.16397406e+47, -1.15250662e+47]
		\\ x_6 & = [1.90007313e+56, 1.88135365e+56]
	\end{align*}
	\item We make a change of coordinates by subtracting $(100,100)$, so that our loss functions becomes $$E(w) = \frac{1}{2} (w_1 + w_2 -2)^2  +\frac{1}{2} (w_1 - w_2) ^2.$$
	We compute:
	$$\grad E(w) = \bmat{  2w_1 - 2\\ 2w_2 - 2  },$$
	$$Q = Hess(E) = 2I.$$
	Therefore at the point $w = (10,10)$, gradient is $\grad E(10,10) = (18,18)$. The learning rate is therefore:
	$$\alpha = \frac{|\grad E(w)|^2}{\grad E(w) Q \grad E(w)} = \frac{1}{2}.$$
	Performing gradient descent gives us 
	$$x_1 = \bmat{10 \\ 10} - \frac{1}{2}\bmat{18 \\ 18} = \bmat{1 \\ 1}$$
	Since the level sets are circles, this must be the global minimum. 
	This can also be seen by noting that $\grad E(1,1) = 0$, so the gradient descent algorthm converges to $(1,1)$. Since this is a quadratic function the minimum is attained and is unique so it must be this $(1,1)$. 
	\epenum
	\newpage
	\begin{problem}
		% problem number 5
	\end{problem}
	Recall the Euler-Lagrange equations: 
	$$ \frac{d}{dx} \left(	\frac{ \partial \eL }{ \partial u^\prime } \right)  = \frac{ \partial \eL }{ \partial u }.$$
	\penum
	\item For $\eL (x,u(x), u^\prime(x))= \frac{ 1 }{ 2 }{u^\prime}^2 (x) - x u(x)$:
	$$ \frac{ d }{ dx } \left( \frac{ \partial \eL }{ \partial u^\prime } \right) = \frac{ d }{ dx }u^\prime(x) = u''(x). $$ 
	$$ \frac{ \partial \eL  }{ \partial u } = -x. $$
	Therefore $$ u''(x) =- x. $$ 
	\item $\eL = u'(x)^2 + u(x)^2 - 2\sin^3(x)$: 	
	$$ \frac{ d }{ dx } \left( \frac{ \partial \eL  }{ \partial u' } \right) = \frac{ d }{ dx }2u'(x) = 2u''(x). $$ 
	$$ \frac{ \partial \eL  }{ \partial u } = 2u(x). $$
	Therefore 
	$$ u''(x) = u(x). $$ 
	\item $\eL = \frac{ \left( u'(x)+1 \right)^2 }{ u^2(x) +1 }$:	
	$$ \frac{ d }{ dx } \left( \frac{ \partial \eL }{ \partial u' } \right) = \frac{ d  }{ dx } 2 \frac{ u'(x)-1 }{ u^2(x) +1 } = 2\frac{u^{\prime \prime}(x) (u^2(x) +1) - (u'(x) -1) 2u(x) u'(x) }{ (u(x)^2+1)^2 }$$
	$$ \frac{ \partial \eL  }{ \partial u } = -\frac{ 2u(x) (u'(x) -1)^2 }{ (u^2(x) +1)^2 }. $$ 
	Therefore:
	$$ u^{\prime \prime}(x) (u^2(x) +1) - (u'(x) -1) 2u(x) u'(x) = - u(x) (u'(x) -1)^2 .$$ 
	\epenum	
\end{document}
