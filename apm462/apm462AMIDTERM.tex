\documentclass[12pt, a4paper]{article}
\usepackage[lmargin =0.5 in, 
rmargin=0.5in, 
tmargin=1in,
bmargin=0.5in]{geometry}
\geometry{letterpaper}
\usepackage{tikz-cd}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{cool}
\usepackage{thmtools}
\usepackage{hyperref}
\graphicspath{ }					%path to an image

%-------- sexy font ------------%
%\usepackage{libertine}
%\usepackage{libertinust1math}

%\usepackage{mlmodern}				% very nice and classic
%\usepackage[utopia]{mathdesign}
%\usepackage[T1]{fontenc}


\usepackage{mlmodern}
\usepackage{eulervm}
%\usepackage{tgtermes} 				%times new roman
%-------- sexy font ------------%


% Problem Styles
%====================================================================%


\newtheorem{problem}{Problem}


\theoremstyle{definition}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{fact}{Fact}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{question}{Question}

\newtheorem{manualprobleminner}{Problem}

\newenvironment{manualproblem}[1]{%
	\renewcommand\themanualprobleminner{#1}%
	\manualprobleminner
}{\endmanualprobleminner}

\newcommand{\penum}{ \begin{enumerate}[label=\bf(\alph*), leftmargin=0pt]}
	\newcommand{\epenum}{ \end{enumerate} }

% Math fonts shortcuts
%====================================================================%

\newcommand{\ring}{\mathcal{R}}
\newcommand{\N}{\mathbb{N}}                           % Natural numbers
\newcommand{\Z}{\mathbb{Z}}                           % Integers
\newcommand{\R}{\mathbb{R}}                           % Real numbers
\newcommand{\C}{\mathbb{C}}                           % Complex numbers
\newcommand{\F}{\mathbb{F}}                           % Arbitrary field
\newcommand{\Q}{\mathbb{Q}}                           % Arbitrary field
\newcommand{\PP}{\mathcal{P}}                         % Partition
\newcommand{\M}{\mathcal{M}}                         % Mathcal M
\newcommand{\eL}{\mathcal{L}}                         % Mathcal L
\newcommand{\T}{\mathbb{T}}                         % Mathcal T
\newcommand{\U}{\mathcal{U}}                         % Mathcal U\\
\newcommand{\V}{\mathcal{V}}                         % Mathcal V

% symbol shortcuts
%====================================================================%

\newcommand{\bd}{\partial}
\newcommand{\grad}{\nabla}
\newcommand{\lam}{\lambda}
\newcommand{\imp}{\implies}
\newcommand{\all}{\forall}
\newcommand{\exs}{\exists}
\newcommand{\delt}{\delta}
\newcommand{\ep}{\varepsilon}
\newcommand{\ra}{\rightarrow}
\newcommand{\vph}{\varphi}

\newcommand{\ol}{\overline}
\newcommand{\f}{\frac}
\newcommand{\lf}{\lfrac}
\newcommand{\df}{\dfrac}

% bracketting shortcuts
%====================================================================%
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\babs}[1]{\Big|#1\Big|}
\newcommand{\bound}{\Big|}
\newcommand{\BB}[1]{\left(#1\right)}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\artanh}{\mathrm{artanh}}
\newcommand{\Med}{\mathrm{Med}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\Range}[1]{\mathrm{range}(#1)}
\newcommand{\Null}[1]{\mathrm{null}(#1)}
\newcommand{\lan}{\langle}
\newcommand{\ran}{\rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inn}[1]{\lan#1\ran}
\newcommand{\op}[1]{\operatorname{#1}}
\newcommand{\bmat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\pmat}[1]{\begin{pmatrix}#1\end{pmatrix}}
\newcommand{\vmat}[1]{\begin{vmatrix}#1\end{vmatrix}}

\newcommand{\amogus}{{\bigcap}\kern-0.8em\raisebox{0.3ex}{$\subset$}}
\newcommand{\Note}{\textbf{Note: }}
\newcommand{\Aside}{{\bf Aside: }}
%restriction
%\newcommand{\op}[1]{\operatorname{#1}}
%\newcommand{\done}{$$\mathcal{QED}$$}

%====================================================================%


\setlength{\parindent}{0pt}      	% No paragraph indentations
\pagestyle{fancy}
\fancyhf{}							% fancy header

\setcounter{secnumdepth}{0}			% sections are numbered but numbers do not appear
\setcounter{tocdepth}{2} 			% no subsubsections in toc

%template
%====================================================================%
%\begin{manualproblem}{1}
%Spivak.
%\end{manualproblem}

%\begin{proof}[Solution]
%\end{proof}

%----------- or -----------%

%\begin{problem} 		
%\end{problem}	

%\penum
%	\item
%\epenum
%====================================================================%


\newcommand{\Course}{APM462}
\newcommand{\hwNumber}{MIDTERM}

%preamble

\title{MIDTERM}
\author{A.N.}
\date{\today}
\lhead{\Course A\hwNumber}
\rhead{\thepage}
%\cfoot{\thepage}


%====================================================================%
\begin{document}

\maketitle

\begin{problem}
\end{problem}
By convexity, we have that $\grad f(x_0) \in \partial f(x_0)$. Conversely, take $w\in \R^n, v \in \bd f(x_0)$. We have that 
$$f(x)  \geq f(x_0) + v^T(x-x_0).$$
Letting $ x = x_0 + sw$, we get
$$ f(x_0 + sw) \geq f(x_0) + s v^Tw.$$
By first order Taylor approximation we can write
$$f(x_0 + sw) = f(x_0) + s \grad f(x_0)^T w + o(s).$$
Rearranging we can write this as 
$$\frac{f(x_0  + sw) - f(x_0)}{s} = \grad f(x_0) \cdot w + \frac{o(s)}{s} \geq v^T w.$$
Taking the limit as $s \to 0$ on both sides we get that 
$$\grad f(x) \cdot w \geq v \cdot w.$$
This is true for all $w$ so in particular we can send $w \to -w$ and get that $$\grad f(x_0)\cdot w = v \cdot w.$$
This implies that $v = \grad f(x_0)$. 
\newpage
\begin{problem}
\end{problem}
We write $C = C_1 \cap C_2$,where $C_1 = \{(x,y): (x-1)^2 + y^2 \leq 1\}$, $C_2 = \{(x,y): x \geq 0\}$. It is a fact that $$\delta_C = \delta_{C_1 \cap C_2} = \delta_{C_1} + \delta_{C_2}.$$ 
Recall that
$$\partial \delta_C(x) = \begin{cases}
0 & \text{$x\in int C$} \\ N_C(x) & \text{$x\in \bd C$}\end{cases}.$$
We first determinal the normal cone of $\bd C_1$. Since $\bd C_1$ is given by the $0$ set of $g(x,y )= (x-1)^2 + y^2 - 1$, the normal cone will be generated by $\lambda \grad g(x,y)$ for $\lambda\geq 0$. Thus:
$$\partial \delta_{C_1}(x) = \begin{cases}
0 & \text{$x\in int C$} \\ \lambda(2x-2, 2y) & \text{$x\in \bd C$}\end{cases}.$$
We now wish to determine the normal cone to $\bd C_2$. For $f(x,y) = -x$, $C_2$ is the set where $f(x,y) \leq 0$. Therefore at the boundary, the gradient is given by $(-1,0)$. Thus $\partial \delta_{C_2}$ is given by:
$$\partial \delta_{C_2}(x) = \begin{cases}
0 & \text{$x\in int C$} \\ (-\lambda, 0) & \text{$x\in \bd C$}\end{cases}.$$
We remark that $\bd C_1 \cap \bd C_2 = \{(0,1), (0,-1)\}$. Since $\partial \delta_C = \partial \delta_{C_1} + \partial \delta_{C_2}.$
Thus we compute: 
$$\partial \delta_{C}(x) = 
\begin{cases}
0 & \text{$x\in int C_1 \cap C_2$}
	\\ \lambda(x-1,y) & \text{$x\in \bd C_1 \cap C_2$}, \lambda \geq 0
	\\(-\lambda , 0) & \text{ $x\in int C_1 \cap \bd C_2$} , \lambda \geq 0
	\\ \lambda_1(-1,0) +\lambda_2 (-1,1) & \text{$x = (0,1)$} , \lambda_1,\lambda_2 \geq 0
	\\ \lambda_1 (-1,0) + \lambda_2(-1,-1) & \text{$x = (0,-1)$}, \lambda_1, \lambda_2 \geq 0
	\\\emptyset & \text{otherwise}

\end{cases}.$$

\newpage
\begin{problem}
\end{problem}
\penum
\item 
Suppose that $v = (v_1,v_2) \in \partial F(x_0,y_0)$. By definition for all $(x,y)$ we have:
$$f(x) + g(y) \geq f(x_0) + g(y_0) + v^T((x,y)-(x_0,y_0)).$$
At the point $(x_0,y)$ we get that 
$$f(x) \geq f(x_0) + v_1^T(x-x_0)\quad (1).$$
Similarly at $(x,y_0)$ we get
$$g(y) \geq g(y_0) + v_2^T(y-y_0) \quad (2).$$
Therefore $v_1 \in \partial f(x_0), v_2\in \partial g(y_0)$. So $v\in \partial f(x_0) \times \partial g(y_0)$. Conversely for $v_1\in \partial f(x_0), v_2 \in \partial g(y_0).$
Simultaneously, $(1),(2)$ hold for all $x,y$. Adding them together, we get
$$f(x) + g(y) \geq f(x_0) + g(y_0) + v^T((x,y) - (x_0,y_0)).$$
So $v = (v_1,v_2) \in \partial F(x_0,y0)$.
\item Since $F(x,y) = |x-3| + |y|$, we apply part $a)$ and compute the subdifferentials of $|x-3|,|y|$ separately. We already know that for $g(y) = |y|$, 
$$\partial g(y) = \begin{cases} 
	-1 & y<0
	\\ [-1,1] & y = 0
	\\ 1 & y >0
\end{cases}.$$
Similarly since $f(x) = |x-3|$ is just a translated $|x|$, we have that 
$$\partial f(x) = \begin{cases} 
	-1 & x<3
	\\ [-1,1] & x = 3
	\\ 1 & x >3
\end{cases}.$$
\epenum
\newpage
\begin{problem}
\end{problem}
\penum
\item Since $g$ is $C^1$, we have that $\partial g(x) = \{\grad g(x) \} = \{(e^x,-1)\}$. We have that $f(x,y) = \max \{|x-2|, |y-3|\}$. We know that $\bd f(x_0) = \ol{co} \left(  \bigcup_i \{\bd f_i(x_0): f_i(x_0) = f(x_0)\}\right)$, where $f_1 (x,y) = |x-2|, f_2(x,y) = |y-3|$. We consider the 3 cases. First note,
	$$\bd f_1 (x,y) = \begin{cases}
		\pmat{-1 \\ 0 } & x<2
		\\ \pmat{[-1,1] \\ 0 } & x = 0
		\\ \pmat{1 \\ 0} &  x >2
	\end{cases} ,
	\quad \bd f_2(x,y) = \begin{cases}
		\pmat{0 \\ -1 } & y<3
		\\ \pmat{0 \\ [-1,1]} & y = 3
		\\ \pmat{0 \\ 1} & y>3
	\end{cases}$$
\begin{enumerate}[label = \Roman*)]
	\item $|x-2| > |y-3|$: While this holds, we have
		$$\partial f(x_0,y_0) = \begin{cases}
			\pmat{1 \\ 0 } & \text{$x_0 >2$}
			\\ \pmat{-1 \\ 0} & \text{ $ x_0 <2$}
		\end{cases}$$
	Since we must have that $|x-2|>0$. 
	\item $|y-3| > |x-2|$: While this holds we have:

		$$\partial f(x_0,y_0) = \begin{cases}
			\pmat{1 \\ 0 } & \text{$y_0 >3$}
			\\ \pmat{-1 \\ 0} & \text{ $ y_0 <3$}
		\end{cases}$$
	\item $|y-3| = |x-2|$: First notice that both sides are $0$ exactly at $(2,3)$. At this point, we have that
	$$\bd f(2,3) = \ol{co} \left\{ \pmat{[-1,1]\\ 0}, \pmat{0\\ [-1,1]} \right\}.$$ 
	Otherwise, we have 4 subcases, where $|x-2| = |y-3| >0$. One of the following must hold true: 
	$$\begin{cases}
	y  = x+1 & x>2, y>3
	\\ y = x+1 & x<2, y<3
	\\ y = -x+5 &x> 2, y<3
	\\ y = -x+5 & x<2, y>3
	\end{cases}.$$
The convex hull in each of these cases will be, respectively,
		$$\begin{cases}
		\pmat{\lambda \\ 1-\lambda} & \lambda \in [0,1] , x>2,y>3
			\\ \pmat{-\lambda \\ \lambda - 1} &\lambda \in [0,1], x<2,y<3
			\\ \pmat{\lambda \\ \lambda - 1} & \lambda \in [0,1], x>2, y<3
			\\ \pmat{-\lambda \\ 1-\lambda} & \lambda \in [0,1], x<2, y>3.
		\end{cases}$$
\end{enumerate}
\item Let $C = \{(x,y): g(x,y) \leq 0\}$. Since $f$ is the maximum of two convex functions it is convex and so $x$ minimizes $f(x)$ over $C$ if and only if $x$ minimizes $f + \delta_C$ over $\R^2$. We first claim that such an $x$ cannot exist on the interiour of $C$. 
	Observe that if $x$ is an interiour point and a minimum, we must have that $0 \in \bd f(x)$. However on the interiour of $C$, $\bd f(x)$ takes one of the following forms: 
	$$\left\{\pmat{-1 \\ 0} \right\} ,\left\{\pmat{0 \\ 1} \right\}, \left\{\pmat{\lambda \\ \lambda - 1} \right\}. $$
Neither of these will be $0$. This leaves a boundary solution. Since the boundary of $C$ is parametrized by a smooth function, the normal cone to the boundary will be given by the normal cone generated by the gradient. 
In particular, $N_c(x) = \lambda \grad g(x) = \lambda (e^x, -1)$. We must find an $x$ so that there exists a $y\in \bd f(x)$, $\lambda \geq 0$ so that $-y \in  \{\lambda(e^x, -1)\}$. 
We can immediately rule out any subgradients of the form $(a,0), (0,a)$ since this will never be in the span of $(e^x,-1)$.
Therefore our candidate points must satisfy either $y=x+1, e^x=y$ or $y= -x+5, e^x = y$, since the subdifferentials along these lines can be within the span of $(e^x, -1)$.
Therefore our candidate points satisfy $e^x = x+1, e^x = -x+5$. We can compute that these will be solved by $(0,1), (1.307,3.693)$. The value of $f$ at these points is $2 , 0.693$. Therefore the global minimum is given by $0.693$ at the point satisfying $-x+5 = e^x$. 
\epenum
\newpage
\begin{problem}
\end{problem}
\penum 
\item We first claim that $g(x) = |a^Tx|$ is a convex function. Indeed for $s\in (0,1)$, by the triangle inequality we have:
	$$g(sx + (1-s)y) = |a^Tsx + a^T(1-s)y| \leq s|a^Tx| + (1-s) |a^Ty| = sg(x) + (1-s) g(y).$$
We now compute it's subgradient. We can write $g(x) = \max \{a^Tx, -a^Tx \}$. This will be smooth away from where $g(x) = 0$, and so the subgradient will be either $a $ or $-a$. When $g(x) = 0$, we have that the subdifferential will be the convex hull of $a,-a$ i.e. $(2t-1) a$ for $t\in [0,1]$. Simply put: 
$$\bd g(x) = \begin{cases}
	-a & a^Tx<0
	\\ (2t-1)a, t\in [0,1] & a^Tx = 0
	\\ a & a^T >0.
\end{cases}$$
\item Let $ C = \{x: g(x) \leq 0\}$. We first claim that the minimum of $f$ occurs on the boundary of $C$. Suppose not. Then $x\in  C$. We must have by the first order conditions that $0 \in \bd f(x)$. We have that 
	$$\bd f =\begin{cases}
	-a & a^Tx<0
	\\ (2t-1)a, t\in [0,1] & a^Tx = 0
	\\ a & a^T >0
\end{cases} + \begin{cases}
	-b & b^Tx<0
	\\ (2t-1)b, t\in [0,1] & b^Tx = 0
	\\ b & b^T >0.
\end{cases} $$
Since $a,b$ are linearly independant we must have that $a^Tx = b^Tx = 0$. Since $C$ is assumed to be disjoint from any such point we obtain a contradiction as desired. Therefore our minimizer must occur on the boundary. We look for an $x$ so that $-y \in \bd g(x)$. The subgradient along the boundary will be the normal cone generated by $\grad g(x) = 2(x-x_0)$.
Note that $y$ must be of the form $ y= \pm a \pm b$, since if it had a $t$ dependance that would correspond to $a^Tx= 0$ or $b^Tx = 0$, which cannot happen for the same reason as mentioned above.
We relabel as $\tilde{a}, \tilde{b}$ so that $\tilde{a}^T x_0 <0, \tilde{b}^Tx_0<0$ by multiplying $a,b$ by $-1$ or $1$. 
First order optimization conditions gives us: 
$$-(-\tilde{a} -\tilde{b}) = \lambda(x-x_0) \implies x = \frac{1}{\lambda} (\tilde{a} + \tilde{b}) + x_0. $$
Putting this into the condition that $g(x) = 0$, we see:
$$r^2 = |x-x_0|^2 \implies r^2 = \frac{1}{\lambda^2} |(\tilde{a} + \tilde{b})|^2 \implies \lambda ^2 = \frac{|\tilde{a} + \tilde{b}|^2}{r^2}.$$
Since $\lambda,r>0$ we have that
$$\lambda =  \frac{|\tilde{a} + \tilde{b}|}{r}.$$
Therefore our candidate minimum is
$$x^* = x_0 + \frac{r}{|\tilde{a} + \tilde{b}|} (\tilde{a} + \tilde{b}). $$
Since $\bd C$ is not convex, and the set of minima form a convex set, the minimum must be exactly $x^*$. 
\epenum
\newpage
\begin{problem}
\end{problem}
\penum
\item First we compute $\partial f(x)$. Since $f$ is smooth, we have that $\partial f(x) = \{\grad f(x)\} = \{2(x-2, y-3)\}$. We see that $0 \in \partial f(x) $ only when $x = (2,3)$, but this is not a feasible point. Therefore a minimum can only occur when $g(x,y)$ is active i.e. $g(x,y) = |x|+|y|- 1 = 0$. By Q3, we know that $\partial g (x,y) = \partial |x| \times \partial |y|$ i.e. :
	$$\partial g(x,y) = \begin{cases} -1 & x<0 \\ [-1,1] & x = 0 \\ 1 & x>0 \end{cases} \times  \begin{cases} -1 & y<0 \\ [-1,1] & y = 0 \\ 1 & y>0 \end{cases}  .$$
A minimum will occur at such $(x,y)$ satisfying $-2(x-2,y-3) \in \lambda \partial g(x,y)$ for $\lambda>0$.
We first rule out and of the cases where $\partial g(x,y) = (\pm 1, \pm 1)$. 
\begin{enumerate}[label = \bf{Case \roman*)}]
	\item $x>0,y>0$. A minimizer must satisfy $-2(x-2,y-3) = (\lambda, \lambda)$. From this it follows that $x-2 = y-3$ and so $y = x+1$. Since $x>0$ this implies that $y>1$, but this is not a feasible point. 
	\item $x>0, y<0$. A minimizer must satisfy $-2(x-2,y-3) = (\lambda , -\lambda)$. It follows that $(x-2) = -(y-3)$ and so $y = -x+5$. Since $y<0$ we see that $x>5$. This is not a feasible point. 
	\item $x<0, y>0$. A minimizer must satisfy $-2(x-2,y-3) = (-\lambda, \lambda)$. From this we have that $(x-2) = -(y-3)$, and so $ y = -x+5$. Since $x<0$ we must have that $y>5$ which is not feasible. 
	\item $x<0, y<0$. A minimizer must satisy $-2(x-2, y-3) = (-\lambda , - \lambda)$. Once again this gives us $x-2 = y-3$ and so $y = x+1$. Since $y<0$ this implies that $x<-1$ which is not feasible. 
\end{enumerate}
Consider $\partial g(0,1)$. This will be of the form $([-1,1], 1)$, and the subdifferential of $f$ at this point is given as $\partial f(0,1) = \{(-4,-4)\}$. We see that by taking $\lambda = 4, (1,1) \in \partial g(0,1)$ that $-(-4,-4) = \lambda(1,1)$ . 
\item We rewrite our optimization problem as: 
	\begin{align*}
	\begin{aligned}
		\min \quad & f(x,y) = (x-2)^2 + (y-3)^2
		\\ \textrm{s.t.} \quad & g_1(x,y) = x+y-1\leq 0
		\\ \quad & g_2(x,y) = x-y-1\leq 0
		\\ \quad & g_3(x,y) = -x+y - 1 \leq 0 
		\\ \quad & g_4(x,y) = -x-y-1\leq 0
	\end{aligned}
	\end{align*}
First order Kuhn-Tucker conditions tell us that a local minimizer $(x,y)$ which is regular will satsify :
$$\grad f (x,y)  + \mu_1 \grad g_1 + \mu_2 \grad g_2 + \mu_3 \grad g_3 + \mu_4 \grad g_4 = 0,$$
For $\mu_i g_i(x,y) = 0$. We can explicitly write this out as: 
$$2 \pmat{x-2 \\ y-3} + \mu_1 \pmat{1 \\ 1 } + \mu_2 \pmat{1 \\ -1} + \mu_3 \pmat{-1 \\ 1} + \mu_4\pmat{-1 \\ -1} = 0.$$
We first check the cases in which only one $g_i$ is active.
\begin{enumerate}[label =\bf{ Case \roman*)}]
	\item $g_1$ active. We have that $y = 1-x$, and 
		$$ 2 \pmat{x-2 \\ y-3} + \pmat{\mu_1 \\ \mu_1} = 0 .$$
		Thus we have that 
		$$ 2 \pmat{x-2 \\ -2-x} + \pmat{\mu_1 \\ \mu_1} = 0.$$ 
		This will be solved by $(x,y) = (0,1), \mu_1 = 4$. 
	\item $g_2$ active. We have that $y = x-1$ and 
$$ 2 \pmat{x-2 \\ y-3} + \pmat{\mu_2 \\- \mu_2} = 0 .$$
Thus we have that 
		$$2 \pmat{x-2 \\ x-4 } + \pmat{\mu_2 \\ -\mu_2} = 0.$$
		This will be solved by $(x,y) = (3,2), \mu_2 = -2$. This is absurd since $\mu_2$ must be positive and $(3,2)$ is not feasible. 
	\item $g_3$ is active. We have that $y = x+1$, and 
		$$2 \pmat{x-2 \\ y-3} + \pmat{-\mu_3 \\ \mu_3} = 0.$$
		We get that 
		$$2 \pmat{x -2 \\ x - 2} + \pmat{-\mu \\ \mu_3} = 0.$$
		This will be solved by $(x,y) = (0,1)$ and $\mu_3 = 4$. 
	\item $g_4$ active. We have that $y = -x-1$, and 
		$$2\pmat{x-2 \\ y-3 } + \pmat{-\mu_4 \\ -\mu_4} = 0. $$
		This gives us
		$$2 \pmat{x-2 \\-x - 4 } + \pmat{-\mu_4 \\ -\mu_4} =0 . $$
		This is solved by $(x,y) = (-1,0)$ and $\mu_4 = -6$. $\mu$ is negative, and so we discard this. 
\end{enumerate}
Therefore we get the candidate point $(0,1)$. It remains to show that this is a regular point. Notice that at this point, we have that $g_1,g_3$ are both active. 
Furthermore, $\grad g_1 = \pmat{1 \\ 1}, \grad g_3 = \pmat{-1 \\ 1}$. These are both linearly independant at $(0,1)$ so this point is indeed regular. 
It remains to check for when two conditions are active i.e. the corners of the shape. 
We can simply just compute the value of $f$ on the corners and select the smallest one as our candidate against $(0,1)$. 
We see: 
$$\begin{cases}
	f(1,0) &= 10
	\\ f(0,1) & = 8
	\\ f(-1,0)  & = 18
	\\ f(0,-1) & = 20.
\end{cases}$$
Our candidate is the unique candidate for a minimum, and since $f$ is convex we conclude that this is in fact the minimum. 
\epenum
\newpage
\begin{problem}
\end{problem}
\penum
\item We first claim that each $f_i(x)$ is convex. For simplicity we verify that $f_1(x)$ is convex. The others will be convex by the same reasoning. Notice that $f_1(x) = |x - A|^2$ is smooth, so it is sufficient to compute it's Hessian. Observe that $\grad f_1(x) = 2(x-A)$ so $H(f) = 2I$.
Thus each $f_i$ is convex. The max of them is therefore convex. 
We now compute $\partial f(x)$. Since the subdifferential is given by the convex hull of the subdifferentials of the active functions,
we consider the following cases: 
\begin{enumerate}[label = \bf{Case \roman*)}]
	\item $f_1 > f_2, f_3$: Since $f_1$ is smooth, we have that $ \partial f (x) = \partial f_1(x) = \{\grad f_1(x)\} = \{2(x-A)\}$
	\item $f_2 > f_1, f_3$:  Since $f_2$ is smooth, we have that $ \partial f (x) = \partial f_2(x) = \{\grad f_2(x)\} = \{2(x-B)\}$
	\item $f_3 < f_1, f_2$: Since $f_3$ is smooth, we have that $ \partial f (x) = \partial f_3(x) = \{\grad f_3(x)\} = \{2(x-C)\}$
	\item $f_1 = f_2 > f_3$: This will be the convex hull of subgradients of the active functions. We have already computed their subgradients individually, all we must do is compute their convex hull:
		$$\partial f(x) = co \{\grad f_1(x) , \grad f_2(x)\} = \{t 2(x-A) + (1-t)2(x-B) : t\in [0,1]\}. $$
	\item $f_3 = f_2 > f_1$: This will be the convex hull of subgradients of the active functions. We have already computed their subgradients individually, all we must do is compute their convex hull:
		$$\partial f(x) = co \{\grad f_3(x) , \grad f_2(x)\} = \{t 2(x-C) + (1-t)2(x-B) : t\in [0,1]\}. $$
	\item $f_1 = f_3 > f_2$: This will be the convex hull of subgradients of the active functions. We have already computed their subgradients individually, all we must do is compute their convex hull:
		$$\partial f(x) = co \{\grad f_1(x) , \grad f_3(x)\} = \{t 2(x-A) + (1-t)2(x-C) : t\in [0,1]\}. $$
	\item $f_1 = f_2 = f_3$: The subdifferential of $f$ at this point will be the convex hull of the subgradients of $f_1, f_2, f_3$: 
		$$\partial f(x) = \{s_1 2(x-A) +2 s_2(x-B) + 2s_3(x-C): s_i\geq 0, s_1+s_2+s_3 = 1\}. $$
\end{enumerate}
\item  A point $x$ is a minimzer of $f$ if and only if $0\in \partial f(x)$. We first claim that this will not occur for cases $i-vi$. First suppose that only one constraint is active at our minimizer. 
For simplicity say that $f_1$ is active. Since $x$ is a minimizer we have that $0 \in \bd f(x)$,
which we know from $a)$ that $0 = 2(x-A)$. Therefore $x=A$. However this means that $f_1(x)= 0$, and since we are on a triangle, $f_2(x), f_3(x) >0$. 
Therefore this cannot happen, and a similar argument for $f_2$ or $f_3$ active yields the same contradiction. Now suppose that 2 constraints are active at our minimum, say $f_1, f_2$. Then we have that $0 \in \{t 2(x-A) + (1-t)2(x-B):t \in [0,1]\}$. Therefore at some $t$ we have that $x = tA + (1-t)B$ i.e. $x$ is on the side $AB$. 
However since $ABC$ is an acute triangle, we must have that $f_3(x) > f_1(x),f_2(x)$. Therefore $f_1,f_2$ cannot be active. 
We apply a similar argument for the other pairs of active functions.
Therefore if a minimum exists, we must have that $f_1 =  f_2 = f_3$ at this point. 
Let $x^\ast$ be the unique point which is equidistant from $A,B,C$. We have that $x^\ast$ lies within the triangle and is therefore a convex linear combination of $A,B,C$. 
We write: 
$$x = t_1A + t_2 B + t_3C.$$
Furthermore, $0 \in\partial f(x^\ast)$ since 
$$2(t_1x + t_2x + t_3 x - t_1 A -t_2 B - t_3 C ) = 0.$$
Therefore $0 \in \partial f(x^\ast)$, and so $x$ is a global minimizer. 
Furthermore, since a minimizer must satisfy $f_1 = f_2 = f_3$ i.e. equidistant from the vertices, and only one such point can exist, we conclude that $x^\ast$ is the unique global minimum. 
\epenum

\end{document}
